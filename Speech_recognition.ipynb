{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Whisper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import soundfile as sf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset \n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from transformers import Seq2SeqTrainingArguments \n",
    "from transformers import Seq2SeqTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are just how those files are named in the loaded data, it has nothing to do with the actual training, validation and testing we know.\n",
    "train_df = pd.read_csv('cv-corpus-10.0-delta-2022-07-04/en/train.csv', on_bad_lines = 'skip', sep='\\t')\n",
    "val_df = pd.read_csv('cv-corpus-10.0-delta-2022-07-04/en/validated.csv', on_bad_lines = 'skip', sep='\\t')\n",
    "test_df = pd.read_csv('cv-corpus-10.0-delta-2022-07-04/en/test.csv', on_bad_lines = 'skip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = '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'\n",
    "gs = load_dataset(\"speechcolab/gigaspeech\", \"xs\", use_auth_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>the following were the members for hastings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>in the afternoon the prussian troops withdrew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>wang went to school in new mexico and nevada.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>john and lydia swift.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>she grew up in the township of pleasant point ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11772</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>he had two daughters with her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12285</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>walls and crowe later divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12299</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>the land which is now the recreation ground wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12497</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>he later became the master in charge of cricke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12547</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>it was also known as outlaw justice.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9590 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  \\\n",
       "0      /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "1      /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "2      /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "3      /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "4      /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "...                                                  ...   \n",
       "11772  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "12285  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "12299  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "12497  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "12547  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "\n",
       "                                                sentence  \n",
       "0           the following were the members for hastings.  \n",
       "1      in the afternoon the prussian troops withdrew ...  \n",
       "2          wang went to school in new mexico and nevada.  \n",
       "3                                  john and lydia swift.  \n",
       "4      she grew up in the township of pleasant point ...  \n",
       "...                                                  ...  \n",
       "11772                     he had two daughters with her.  \n",
       "12285                    walls and crowe later divorced.  \n",
       "12299  the land which is now the recreation ground wa...  \n",
       "12497  he later became the master in charge of cricke...  \n",
       "12547               it was also known as outlaw justice.  \n",
       "\n",
       "[9590 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the columns we don't need\n",
    "train_df = train_df[['path', 'sentence']]\n",
    "val_df = val_df[['path', 'sentence']]\n",
    "test_df = test_df[['path', 'sentence']]\n",
    "\n",
    "# concatenating the dataframes so we can do the train_test_split\n",
    "df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "df = df.drop_duplicates(subset=['path'])\n",
    "\n",
    "# parsing sentences\n",
    "df['sentence'] = df['sentence'].str.lower()\n",
    "\n",
    "prefix = '/Users/alijanatiidr/Desktop/Columbia/Applied ML/Project/cv-corpus-10.0-delta-2022-07-04/en/clips/'\n",
    "\n",
    "df['path'] = prefix + df['path']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_train = pd.DataFrame(gs['train'])\n",
    "gs_test = pd.DataFrame(gs['test'])\n",
    "gs_val = pd.DataFrame(gs['validation'])\n",
    "\n",
    "gs = pd.concat([gs_train, gs_test, gs_val], ignore_index=True)\n",
    "\n",
    "gs = gs[['audio', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gs.rename(columns={'audio': 'audio_signal', 'text': 'sentence'})\n",
    "#we are only going to take the first 2000 data points since we don't have any GPU for our training\n",
    "gs = gs[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting fine tuning dataset of the form (path of mp3, sentence)\n",
    "gs = gs.rename(columns={'audio_signal': 'path', 'sentence': 'text'})\n",
    "gs['path'] = gs['path'].apply(lambda x: x['path'])\n",
    "\n",
    "gs['text'] = gs['text'].str.lower()\n",
    "gs['text'] = gs['text'].str.replace('<comma>', ',')\n",
    "gs['text'] = gs['text'].str.replace('<period>', '.')\n",
    "gs['text'] = gs['text'].str.replace('<questionmark>', '?')\n",
    "gs['text'] = gs['text'].str.replace('<exclamationmark>', '!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'audio_signal': 'path', 'sentence': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_df = pd.concat([gs, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to audio signal using librosa\n",
    "def read_audio_signal(row):\n",
    "    path = row['path']\n",
    "    try:\n",
    "        audio_signal, sample_rate = sf.read(path)\n",
    "        return audio_signal\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that might occur during file reading\n",
    "        print(f\"Error reading file '{path}': {e}\")\n",
    "        return None\n",
    "\n",
    "fine_tuning_df['audio'] = fine_tuning_df.apply(read_audio_signal, axis=1)\n",
    "\n",
    "fine_tuning_df['sampling_rate'] = 16000\n",
    "\n",
    "fine_tuning_df = fine_tuning_df[['path', 'audio', 'sampling_rate' , 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>audio</th>\n",
       "      <th>sampling_rate</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/alijanatiidr/.cache/huggingface/dataset...</td>\n",
       "      <td>[0.000518798828125, 0.0008544921875, 0.0001220...</td>\n",
       "      <td>16000</td>\n",
       "      <td>as they're leaving , can kash pull zahra aside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/alijanatiidr/.cache/huggingface/dataset...</td>\n",
       "      <td>[0.001434326171875, 0.001373291015625, 0.00131...</td>\n",
       "      <td>16000</td>\n",
       "      <td>six tomatoes .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/alijanatiidr/.cache/huggingface/dataset...</td>\n",
       "      <td>[-0.000457763671875, -0.000335693359375, -0.00...</td>\n",
       "      <td>16000</td>\n",
       "      <td>and something brought back restored from the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/alijanatiidr/.cache/huggingface/dataset...</td>\n",
       "      <td>[0.000213623046875, 0.0003662109375, 0.0005493...</td>\n",
       "      <td>16000</td>\n",
       "      <td>to help screen reader users in the midst of di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/alijanatiidr/.cache/huggingface/dataset...</td>\n",
       "      <td>[0.006195068359375, 0.0052490234375, 0.0039672...</td>\n",
       "      <td>16000</td>\n",
       "      <td>for alice had read several nice little stories...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11585</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>[0.0, 1.7661124299128694e-12, 1.56589411576257...</td>\n",
       "      <td>16000</td>\n",
       "      <td>he had two daughters with her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>[0.0, -8.515981160268221e-15, -4.4422456571503...</td>\n",
       "      <td>16000</td>\n",
       "      <td>walls and crowe later divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11587</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>[0.0, 6.933249488104963e-16, -8.30595425879883...</td>\n",
       "      <td>16000</td>\n",
       "      <td>the land which is now the recreation ground wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11588</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>[0.0, -2.5103812258120417e-13, -2.844536238499...</td>\n",
       "      <td>16000</td>\n",
       "      <td>he later became the master in charge of cricke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11589</th>\n",
       "      <td>/Users/alijanatiidr/Desktop/Columbia/Applied M...</td>\n",
       "      <td>[0.0, -1.1388611408091887e-13, 3.4422101670775...</td>\n",
       "      <td>16000</td>\n",
       "      <td>it was also known as outlaw justice.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11590 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  \\\n",
       "0      /Users/alijanatiidr/.cache/huggingface/dataset...   \n",
       "1      /Users/alijanatiidr/.cache/huggingface/dataset...   \n",
       "2      /Users/alijanatiidr/.cache/huggingface/dataset...   \n",
       "3      /Users/alijanatiidr/.cache/huggingface/dataset...   \n",
       "4      /Users/alijanatiidr/.cache/huggingface/dataset...   \n",
       "...                                                  ...   \n",
       "11585  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "11586  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "11587  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "11588  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "11589  /Users/alijanatiidr/Desktop/Columbia/Applied M...   \n",
       "\n",
       "                                                   audio  sampling_rate  \\\n",
       "0      [0.000518798828125, 0.0008544921875, 0.0001220...          16000   \n",
       "1      [0.001434326171875, 0.001373291015625, 0.00131...          16000   \n",
       "2      [-0.000457763671875, -0.000335693359375, -0.00...          16000   \n",
       "3      [0.000213623046875, 0.0003662109375, 0.0005493...          16000   \n",
       "4      [0.006195068359375, 0.0052490234375, 0.0039672...          16000   \n",
       "...                                                  ...            ...   \n",
       "11585  [0.0, 1.7661124299128694e-12, 1.56589411576257...          16000   \n",
       "11586  [0.0, -8.515981160268221e-15, -4.4422456571503...          16000   \n",
       "11587  [0.0, 6.933249488104963e-16, -8.30595425879883...          16000   \n",
       "11588  [0.0, -2.5103812258120417e-13, -2.844536238499...          16000   \n",
       "11589  [0.0, -1.1388611408091887e-13, 3.4422101670775...          16000   \n",
       "\n",
       "                                                    text  \n",
       "0      as they're leaving , can kash pull zahra aside...  \n",
       "1                                         six tomatoes .  \n",
       "2      and something brought back restored from the r...  \n",
       "3      to help screen reader users in the midst of di...  \n",
       "4      for alice had read several nice little stories...  \n",
       "...                                                  ...  \n",
       "11585                     he had two daughters with her.  \n",
       "11586                    walls and crowe later divorced.  \n",
       "11587  the land which is now the recreation ground wa...  \n",
       "11588  he later became the master in charge of cricke...  \n",
       "11589               it was also known as outlaw justice.  \n",
       "\n",
       "[11590 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "fine_tuning_df_train = fine_tuning_df[:int(len(fine_tuning_df)*train_size)]\n",
    "fine_tuning_df_val = fine_tuning_df[int(len(fine_tuning_df)*train_size):int(len(fine_tuning_df)*(train_size+val_size))]\n",
    "fine_tuning_df_test = fine_tuning_df[int(len(fine_tuning_df)*(train_size+val_size)):]\n",
    "\n",
    "fine_tuning_df_train = fine_tuning_df_train.reset_index(drop=True)\n",
    "fine_tuning_df_val = fine_tuning_df_val.reset_index(drop=True)\n",
    "fine_tuning_df_test = fine_tuning_df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = DatasetDict({'train': Dataset.from_pandas(fine_tuning_df_train), 'validation': Dataset.from_pandas(fine_tuning_df_val), 'test': Dataset.from_pandas(fine_tuning_df_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.remove_columns(['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sampling_rate', 'text'],\n",
       "        num_rows: 9272\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sampling_rate', 'text'],\n",
       "        num_rows: 1159\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'sampling_rate', 'text'],\n",
       "        num_rows: 1159\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Assessment without fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, AutoTokenizer\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/whisper-tiny.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wer metric\n",
    "from datasets import load_metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "test_predictions = []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in Data['test']:\n",
    "        input_features = processor(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        generated_ids = model.generate(input_features.input_features)\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        test_predictions.append(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "for sentence in Data['test']['text']:\n",
    "    test_text.append([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [item for sublist in test_text for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 2.92\n"
     ]
    }
   ],
   "source": [
    "wer = wer_metric.compute(predictions=test_predictions, references=test_text)\n",
    "print(\"WER: {}\".format(round(wer, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER: 2.2840200699407025\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "cer_whisper_tiny = cer_metric.compute(predictions=test_predictions, references=test_text)\n",
    "\n",
    "print(f\"CER: {cer_whisper_tiny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37760256"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numbers of parameters in the model\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper Large v3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load whisper large v3 model\n",
    "processor_large_v3 = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "model_large_v3 = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v3\")\n",
    "tokenizer_large_v3 = AutoTokenizer.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "test_predictions_large_v3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in Data['test']:\n",
    "        input_features = processor_large_v3(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        generated_ids = model_large_v3.generate(input_features.input_features)\n",
    "        transcription = processor_large_v3.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        test_predictions_large_v3.append(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_large_v3_list = [item for sublist in test_predictions_large_v3 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.49178069611534575\n",
      "CER: 0.2261213319142466\n"
     ]
    }
   ],
   "source": [
    "# wer score\n",
    "wer_whisper_large_v3 = wer_metric.compute(predictions=test_predictions_large_v3_list, references=test_text)\n",
    "\n",
    "# cer score\n",
    "cer_whisper_large_v3 = cer_metric.compute(predictions=test_predictions_large_v3_list, references=test_text)\n",
    "\n",
    "print(f\"WER: {wer_whisper_large_v3}\")\n",
    "\n",
    "print(f\"CER: {cer_whisper_large_v3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1543490560"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numbers of parameters in the model\n",
    "model_large_v3.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper medium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0b889e86a04790acaa214bb4ff9351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc45bd99c94e4890b9c0c7ef006133b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b587b43838c84a3c98a55b49a9db56f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea198ae75584fe08f0cbb6eb83d8fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9bdc9b19874c20a1edb400743e0d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f371b3a8b742eaaaf84d8b3e051210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2d5e8edf6746959f0f5fde64f3f456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddcf06ef8484a7592091e878e08941e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35563ad6e33f429abac8e9ec04ec432f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2a908834694962b7c05dcc6be21915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23116ceae584cd9bd575b33fa803c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load whisper medium model\n",
    "processor_medium = AutoProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "model_medium = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-medium\")\n",
    "tokenizer_medium = AutoTokenizer.from_pretrained(\"openai/whisper-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "test_predictions_medium = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in Data['test']:\n",
    "        input_features = processor_medium(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        generated_ids = model_medium.generate(input_features.input_features)\n",
    "        transcription = processor_medium.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        test_predictions_medium.append(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wer score\n",
    "wer_whisper_medium = wer_metric.compute(predictions=test_predictions_medium, references=test_text)\n",
    "\n",
    "# cer score\n",
    "cer_whisper_medium = cer_metric.compute(predictions=test_predictions_medium, references=test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.563779961428965\n",
      "CER: 0.27809031473316104\n"
     ]
    }
   ],
   "source": [
    "print(f\"WER: {wer_whisper_medium}\")\n",
    "\n",
    "print(f\"CER: {cer_whisper_medium}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763857920"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters in the model\n",
    "model_medium.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca35134cad44a1aa9da2b473d424944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef30f191460489c9a68d982e3284bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62629d125b845cfa401fee9923ebdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1419cf770cf4b74967ef7973b3c4ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf895a42f7f4b2a9947699eb39cd87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0562468b6409451896b37608c76d3982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d870ceaeef42b086fb1935afba72cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862613a1d1fc4fe6b283677cb21d3675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43bb95e44ef4a09a7cf45712c6f63fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7612f20eb03349468d6745303d2178bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8c4b30215a41b3a67208136cb38c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load whisper base model\n",
    "processor_base = AutoProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "model_base = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-base\")\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "test_predictions_base = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in Data['test']:\n",
    "        input_features = processor_base(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        generated_ids = model_base.generate(input_features.input_features)\n",
    "        transcription = processor_base.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        test_predictions_base.append(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# wer score\n",
    "wer_whisper_base = wer_metric.compute(predictions=test_predictions_base, references=test_text)\n",
    "\n",
    "# cer score\n",
    "cer_whisper_base = cer_metric.compute(predictions=test_predictions_base, references=test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 2.270089080723666\n",
      "CER: 1.618823171658811\n"
     ]
    }
   ],
   "source": [
    "print(f\"WER: {wer_whisper_base}\")\n",
    "\n",
    "print(f\"CER: {cer_whisper_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72593920"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters in the model\n",
    "model_base.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348a2bccc62046fda11695a5c35d4256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522141d6dc62435da5588014232449ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7aef6ca6d049c8826b21d96b8efbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0c872bd1ab4bd2b8424b676a820c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bc1b56fc9c4d7da0117dc26c3b9692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f657fa20454fa2806288106a31c2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5da2043c1d042b2bf638c731cd62c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0711c51b00124d518507745ba60f680a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b7b5287651456b9498af6cd02d9ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c684cec178114b26a07be0f49aaf968a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c4bd239536491bacdadcb2e7c7ff73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load whisper-small.en model\n",
    "processor_small = AutoProcessor.from_pretrained(\"openai/whisper-small.en\")\n",
    "model_small = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small.en\")\n",
    "tokenizer_small = AutoTokenizer.from_pretrained(\"openai/whisper-small.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "test_predictions_small = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in Data['test']:\n",
    "        input_features = processor_small(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        generated_ids = model_small.generate(input_features.input_features)\n",
    "        transcription = processor_small.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        test_predictions_small.append(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.8979704288731748\n",
      "CER: 0.5407784704272465\n"
     ]
    }
   ],
   "source": [
    "# wer score\n",
    "wer_whisper_small = wer_metric.compute(predictions=test_predictions_small, references=test_text)\n",
    "\n",
    "# cer score\n",
    "cer_whisper_small = cer_metric.compute(predictions=test_predictions_small, references=test_text)\n",
    "\n",
    "print(f\"WER: {wer_whisper_small}\")\n",
    "\n",
    "print(f\"CER: {cer_whisper_small}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241734144"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters in the model\n",
    "model_small.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "WER",
         "type": "scatter",
         "x": [
          37760256,
          1543490560,
          763857920,
          72593920,
          241734144
         ],
         "y": [
          2.924694645973,
          0.49178069611534575,
          0.563779961428965,
          2.270089080723666,
          0.8979704288731748
         ]
        },
        {
         "mode": "markers",
         "name": "CER",
         "type": "scatter",
         "x": [
          37760256,
          1543490560,
          763857920,
          72593920,
          241734144
         ],
         "y": [
          2.2840200699407025,
          0.2261213319142466,
          0.27809031473316104,
          1.618823171658811,
          0.5407784704272465
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "WER and CER vs Number of Parameters"
        },
        "xaxis": {
         "title": {
          "text": "Number of Parameters"
         }
        },
        "yaxis": {
         "title": {
          "text": "WER and CER"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot wer and cer scores vs number of parameters\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Line(x=[model.num_parameters(), model_large_v3.num_parameters(), model_medium.num_parameters(), model_base.num_parameters(), model_small.num_parameters()], y=[wer, wer_whisper_large_v3, wer_whisper_medium, wer_whisper_base, wer_whisper_small], mode='markers', name='WER'))\n",
    "fig.add_trace(go.Line(x=[model.num_parameters(), model_large_v3.num_parameters(), model_medium.num_parameters(), model_base.num_parameters(), model_small.num_parameters()], y=[cer_whisper_tiny, cer_whisper_large_v3, cer_whisper_medium, cer_whisper_base, cer_whisper_small], mode='markers', name='CER'))\n",
    "\n",
    "fig.update_layout(title='WER and CER vs Number of Parameters', xaxis_title='Number of Parameters', yaxis_title='WER and CER')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance assessment summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Model', 'WER', 'CER', 'Number of parameters'])\n",
    "df = df.append({'Model': 'Whisper-Tiny', 'WER': wer, 'CER': cer_whisper_tiny, 'Number of parameters': model.num_parameters()}, ignore_index=True)\n",
    "df = df.append({'Model': 'Whisper-Large-V3', 'WER': wer_whisper_large_v3, 'CER': cer_whisper_large_v3, 'Number of parameters': model_large_v3.num_parameters()}, ignore_index=True)\n",
    "df = df.append({'Model': 'Whisper-Medium', 'WER': wer_whisper_medium, 'CER': cer_whisper_medium, 'Number of parameters': model_medium.num_parameters()}, ignore_index=True)\n",
    "df = df.append({'Model': 'Whisper-Base', 'WER': wer_whisper_base, 'CER': cer_whisper_base, 'Number of parameters': model_base.num_parameters()}, ignore_index=True)\n",
    "df = df.append({'Model': 'Whisper-Small', 'WER': wer_whisper_small, 'CER': cer_whisper_small, 'Number of parameters': model_small.num_parameters()}, ignore_index=True)\n",
    "\n",
    "# order by number of parameters\n",
    "df = df.sort_values(by=['Number of parameters'])\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning whisper tiny on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class whisper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "    \n",
    "    def forward(self, input_features, decoder_input_ids):\n",
    "        return self.model(input_features, decoder_input_ids=decoder_input_ids)\n",
    "    \n",
    "    def generate(self, input_features):\n",
    "        return self.model.generate(input_features)\n",
    "\n",
    "\n",
    "    def process_audio(self, audio):\n",
    "        return self.processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        return self.tokenizer(text, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37184256\n"
     ]
    }
   ],
   "source": [
    "# get number of parameters\n",
    "model = whisper()\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(Data['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valloader = torch.utils.data.DataLoader(Data['validation'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    model.train()\n",
    "    batch_train_loss = []\n",
    "    for batch in trainloader:\n",
    "        input_features = batch[0]\n",
    "        input_features = model.process_audio(input_features)\n",
    "        labels = batch[1]\n",
    "        labels = model.tokenize_text(labels)\n",
    "        outputs = model(input_features, decoder_input_ids=labels)\n",
    "        loss = outputs.loss\n",
    "        batch_train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss.append(sum(batch_train_loss)/len(batch_train_loss))\n",
    "    print(f\"Training loss: {train_loss[-1]}\")\n",
    "    model.eval()\n",
    "    batch_val_loss = []\n",
    "    for batch in valloader:\n",
    "        input_features = batch[0]\n",
    "        labels = batch[1]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_features, decoder_input_ids=labels)\n",
    "            loss = outputs.loss\n",
    "            batch_val_loss.append(loss.item())\n",
    "    val_loss.append(sum(batch_val_loss)/len(batch_val_loss))\n",
    "    print(f\"Validation loss: {val_loss[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
