{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Whisper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import soundfile as sf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset \n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from transformers import Seq2SeqTrainingArguments \n",
    "from transformers import Seq2SeqTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are just how those files are named in the loaded data, it has nothing to do with the actual training, validation and testing we know.\n",
    "#train_df = pd.read_csv('cv-corpus-10.0-delta-2022-07-04/en/train.csv', on_bad_lines = 'skip', sep='\\t')\n",
    "#val_df = pd.read_csv('cv-corpus-10.0-delta-2022-07-04/en/validated.csv', on_bad_lines = 'skip', sep='\\t')\n",
    "#test_df = pd.read_csv('cv-corpus-10.0-delta-2022-07-04/en/test.csv', on_bad_lines = 'skip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset gigaspeech (C:/Users/Pc/.cache/huggingface/datasets/speechcolab___gigaspeech/xs/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00722848ad74485383fd846ac45e1f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token = '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'\n",
    "gs = load_dataset(\"speechcolab/gigaspeech\", \"xs\", use_auth_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns we don't need\n",
    "#train_df = train_df[['path', 'sentence']]\n",
    "#val_df = val_df[['path', 'sentence']]\n",
    "#test_df = test_df[['path', 'sentence']]\n",
    "\n",
    "# concatenating the dataframes so we can do the train_test_split\n",
    "#df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "#df = df.drop_duplicates(subset=['path'])\n",
    "\n",
    "# parsing sentences\n",
    "#df['sentence'] = df['sentence'].str.lower()\n",
    "\n",
    "#prefix = '/Users/alijanatiidr/Desktop/Columbia/Applied ML/Project/cv-corpus-10.0-delta-2022-07-04/en/clips/'\n",
    "\n",
    "#df['path'] = prefix + df['path']\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_train = pd.DataFrame(gs['train'])\n",
    "gs_test = pd.DataFrame(gs['test'])\n",
    "gs_val = pd.DataFrame(gs['validation'])\n",
    "\n",
    "gs = pd.concat([gs_train, gs_test, gs_val], ignore_index=True)\n",
    "\n",
    "gs = gs[['audio', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>AS THEY'RE LEAVING &lt;COMMA&gt; CAN KASH PULL ZAHRA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>SIX TOMATOES &lt;PERIOD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>AND SOMETHING BROUGHT BACK RESTORED FROM THE R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>TO HELP SCREEN READER USERS IN THE MIDST OF DI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>FOR ALICE HAD READ SEVERAL NICE LITTLE STORIES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41753</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>THEIR ORIGINAL IDEA IS NOT MEAL KITS &lt;PERIOD&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41754</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>&lt;SIL&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41755</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>ANGUILLA &lt;PERIOD&gt; WHAT MAKES ANGUILLA'S BEACHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41756</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>HAS ANYBODY &lt;COMMA&gt; DOES ANYBODY HAVE IDEA WHY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41757</th>\n",
       "      <td>{'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...</td>\n",
       "      <td>THEN YOU CAN LOOK AT O B S OR WIRECAST AS A PR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41758 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   audio  \\\n",
       "0      {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "1      {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "2      {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "3      {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "4      {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "...                                                  ...   \n",
       "41753  {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "41754  {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "41755  {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "41756  {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "41757  {'path': 'C:\\Users\\Pc\\.cache\\huggingface\\datas...   \n",
       "\n",
       "                                                    text  \n",
       "0      AS THEY'RE LEAVING <COMMA> CAN KASH PULL ZAHRA...  \n",
       "1                                  SIX TOMATOES <PERIOD>  \n",
       "2      AND SOMETHING BROUGHT BACK RESTORED FROM THE R...  \n",
       "3      TO HELP SCREEN READER USERS IN THE MIDST OF DI...  \n",
       "4      FOR ALICE HAD READ SEVERAL NICE LITTLE STORIES...  \n",
       "...                                                  ...  \n",
       "41753  THEIR ORIGINAL IDEA IS NOT MEAL KITS <PERIOD> ...  \n",
       "41754                                              <SIL>  \n",
       "41755  ANGUILLA <PERIOD> WHAT MAKES ANGUILLA'S BEACHE...  \n",
       "41756  HAS ANYBODY <COMMA> DOES ANYBODY HAVE IDEA WHY...  \n",
       "41757  THEN YOU CAN LOOK AT O B S OR WIRECAST AS A PR...  \n",
       "\n",
       "[41758 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gs.rename(columns={'audio': 'audio_signal', 'text': 'sentence'})\n",
    "#we are only going to take the first 10000 data points since we don't have any GPU for our training\n",
    "gs = gs[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting fine tuning dataset of the form (path of mp3, sentence)\n",
    "gs = gs.rename(columns={'audio_signal': 'path', 'sentence': 'text'})\n",
    "gs['path'] = gs['path'].apply(lambda x: x['path'])\n",
    "\n",
    "gs['text'] = gs['text'].str.lower()\n",
    "gs['text'] = gs['text'].str.replace('<comma>', ',')\n",
    "gs['text'] = gs['text'].str.replace('<period>', '.')\n",
    "gs['text'] = gs['text'].str.replace('<questionmark>', '?')\n",
    "gs['text'] = gs['text'].str.replace('<exclamationmark>', '!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.rename(columns={'audio_signal': 'path', 'sentence': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_df = gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to audio signal using librosa\n",
    "def read_audio_signal(row):\n",
    "    path = row['path']\n",
    "    try:\n",
    "        audio_signal, sample_rate = sf.read(path)\n",
    "        return audio_signal\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that might occur during file reading\n",
    "        print(f\"Error reading file '{path}': {e}\")\n",
    "        return None\n",
    "\n",
    "fine_tuning_df['audio'] = fine_tuning_df.apply(read_audio_signal, axis=1)\n",
    "\n",
    "fine_tuning_df['sampling_rate'] = 16000\n",
    "\n",
    "fine_tuning_df = fine_tuning_df[['path', 'audio', 'sampling_rate' , 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>audio</th>\n",
       "      <th>sampling_rate</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[0.000518798828125, 0.0008544921875, 0.0001220...</td>\n",
       "      <td>16000</td>\n",
       "      <td>as they're leaving , can kash pull zahra aside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[0.001434326171875, 0.001373291015625, 0.00131...</td>\n",
       "      <td>16000</td>\n",
       "      <td>six tomatoes .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[-0.000457763671875, -0.000335693359375, -0.00...</td>\n",
       "      <td>16000</td>\n",
       "      <td>and something brought back restored from the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[0.000213623046875, 0.0003662109375, 0.0005493...</td>\n",
       "      <td>16000</td>\n",
       "      <td>to help screen reader users in the midst of di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[0.006195068359375, 0.0052490234375, 0.0039672...</td>\n",
       "      <td>16000</td>\n",
       "      <td>for alice had read several nice little stories...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[-0.069061279296875, -0.041534423828125, -0.04...</td>\n",
       "      <td>16000</td>\n",
       "      <td>writer director eugene ashe combines romantic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[0.00299072265625, 0.0023193359375, 0.00128173...</td>\n",
       "      <td>16000</td>\n",
       "      <td>right .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[-0.001678466796875, -0.00091552734375, -0.000...</td>\n",
       "      <td>16000</td>\n",
       "      <td>in fact , i made up my mind to find a career t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "      <td>would've done something like that .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...</td>\n",
       "      <td>[0.00738525390625, 0.012725830078125, 0.017120...</td>\n",
       "      <td>16000</td>\n",
       "      <td>line judge right there watching look at the ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  \\\n",
       "0     C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "1     C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "2     C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "3     C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "4     C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "...                                                 ...   \n",
       "9995  C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "9996  C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "9997  C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "9998  C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "9999  C:\\Users\\Pc\\.cache\\huggingface\\datasets\\downlo...   \n",
       "\n",
       "                                                  audio  sampling_rate  \\\n",
       "0     [0.000518798828125, 0.0008544921875, 0.0001220...          16000   \n",
       "1     [0.001434326171875, 0.001373291015625, 0.00131...          16000   \n",
       "2     [-0.000457763671875, -0.000335693359375, -0.00...          16000   \n",
       "3     [0.000213623046875, 0.0003662109375, 0.0005493...          16000   \n",
       "4     [0.006195068359375, 0.0052490234375, 0.0039672...          16000   \n",
       "...                                                 ...            ...   \n",
       "9995  [-0.069061279296875, -0.041534423828125, -0.04...          16000   \n",
       "9996  [0.00299072265625, 0.0023193359375, 0.00128173...          16000   \n",
       "9997  [-0.001678466796875, -0.00091552734375, -0.000...          16000   \n",
       "9998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          16000   \n",
       "9999  [0.00738525390625, 0.012725830078125, 0.017120...          16000   \n",
       "\n",
       "                                                   text  \n",
       "0     as they're leaving , can kash pull zahra aside...  \n",
       "1                                        six tomatoes .  \n",
       "2     and something brought back restored from the r...  \n",
       "3     to help screen reader users in the midst of di...  \n",
       "4     for alice had read several nice little stories...  \n",
       "...                                                 ...  \n",
       "9995  writer director eugene ashe combines romantic ...  \n",
       "9996                                            right .  \n",
       "9997  in fact , i made up my mind to find a career t...  \n",
       "9998                would've done something like that .  \n",
       "9999  line judge right there watching look at the ha...  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "fine_tuning_df_train = fine_tuning_df[:int(len(fine_tuning_df)*train_size)]\n",
    "fine_tuning_df_val = fine_tuning_df[int(len(fine_tuning_df)*train_size):int(len(fine_tuning_df)*(train_size+val_size))]\n",
    "fine_tuning_df_test = fine_tuning_df[int(len(fine_tuning_df)*(train_size+val_size)):]\n",
    "\n",
    "fine_tuning_df_train = fine_tuning_df_train.reset_index(drop=True)\n",
    "fine_tuning_df_val = fine_tuning_df_val.reset_index(drop=True)\n",
    "fine_tuning_df_test = fine_tuning_df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = DatasetDict({'train': Dataset.from_pandas(fine_tuning_df_train), 'validation': Dataset.from_pandas(fine_tuning_df_val), 'test': Dataset.from_pandas(fine_tuning_df_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.remove_columns(['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sampling_rate', 'text'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sampling_rate', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'sampling_rate', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Assessment without fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, AutoTokenizer\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/whisper-tiny.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wer metric\n",
    "from datasets import load_metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "test_predictions = []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in Data['test']:\n",
    "        input_features = processor(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        generated_ids = model.generate(input_features.input_features)\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        test_predictions.append(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "for sentence in Data['test']['text']:\n",
    "    test_text.append([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [item for sublist in test_text for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.45\n"
     ]
    }
   ],
   "source": [
    "wer = wer_metric.compute(predictions=test_predictions, references=test_text)\n",
    "print(\"WER: {}\".format(round(wer, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER: 0.2038523909346201\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "cer_whisper_tiny = cer_metric.compute(predictions=test_predictions, references=test_text)\n",
    "\n",
    "print(f\"CER: {cer_whisper_tiny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37760256"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numbers of parameters in the model\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper Large v3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load whisper large v3 model\n",
    "#processor_large_v3 = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "#model_large_v3 = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v3\")\n",
    "#tokenizer_large_v3 = AutoTokenizer.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "#test_predictions_large_v3 = []\n",
    "\n",
    "#with torch.no_grad():\n",
    "    #for example in Data['test']:\n",
    "        #input_features = processor_large_v3(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        #generated_ids = model_large_v3.generate(input_features.input_features)\n",
    "        #transcription = processor_large_v3.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        #test_predictions_large_v3.append(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_predictions_large_v3_list = [item for sublist in test_predictions_large_v3 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wer score\n",
    "#wer_whisper_large_v3 = wer_metric.compute(predictions=test_predictions_large_v3_list, references=test_text)\n",
    "\n",
    "# cer score\n",
    "#cer_whisper_large_v3 = cer_metric.compute(predictions=test_predictions_large_v3_list, references=test_text)\n",
    "\n",
    "#print(f\"WER: {wer_whisper_large_v3}\")\n",
    "\n",
    "#print(f\"CER: {cer_whisper_large_v3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers of parameters in the model\n",
    "#model_large_v3.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper medium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load whisper medium model\n",
    "#processor_medium = AutoProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "#model_medium = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-medium\")\n",
    "#tokenizer_medium = AutoTokenizer.from_pretrained(\"openai/whisper-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "#test_predictions_medium = []\n",
    "\n",
    "#with torch.no_grad():\n",
    "    #for example in Data['test']:\n",
    "        #input_features = processor_medium(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        #generated_ids = model_medium.generate(input_features.input_features)\n",
    "        #transcription = processor_medium.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        #test_predictions_medium.append(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wer score\n",
    "#wer_whisper_medium = wer_metric.compute(predictions=test_predictions_medium, references=test_text)\n",
    "\n",
    "# cer score\n",
    "#cer_whisper_medium = cer_metric.compute(predictions=test_predictions_medium, references=test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"WER: {wer_whisper_medium}\")\n",
    "\n",
    "#print(f\"CER: {cer_whisper_medium}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters in the model\n",
    "#model_medium.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load whisper base model\n",
    "#processor_base = AutoProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "#model_base = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-base\")\n",
    "#tokenizer_base = AutoTokenizer.from_pretrained(\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "#test_predictions_base = []\n",
    "\n",
    "\n",
    "#with torch.no_grad():\n",
    "    #for example in Data['test']:\n",
    "        #input_features = processor_base(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        #generated_ids = model_base.generate(input_features.input_features)\n",
    "        #transcription = processor_base.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        #test_predictions_base.append(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# wer score\n",
    "#wer_whisper_base = wer_metric.compute(predictions=test_predictions_base, references=test_text)\n",
    "\n",
    "# cer score\n",
    "#cer_whisper_base = cer_metric.compute(predictions=test_predictions_base, references=test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"WER: {wer_whisper_base}\")\n",
    "\n",
    "#print(f\"CER: {cer_whisper_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters in the model\n",
    "#model_base.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load whisper-small.en model\n",
    "#processor_small = AutoProcessor.from_pretrained(\"openai/whisper-small.en\")\n",
    "#model_small = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small.en\")\n",
    "#tokenizer_small = AutoTokenizer.from_pretrained(\"openai/whisper-small.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test set\n",
    "#test_predictions_small = []\n",
    "\n",
    "\n",
    "#with torch.no_grad():\n",
    "    #for example in Data['test']:\n",
    "        #input_features = processor_small(example['audio'], sampling_rate=example['sampling_rate'], return_tensors=\"pt\")\n",
    "        #generated_ids = model_small.generate(input_features.input_features)\n",
    "        #transcription = processor_small.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        #test_predictions_small.append(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wer score\n",
    "#wer_whisper_small = wer_metric.compute(predictions=test_predictions_small, references=test_text)\n",
    "\n",
    "# cer score\n",
    "#cer_whisper_small = cer_metric.compute(predictions=test_predictions_small, references=test_text)\n",
    "\n",
    "#print(f\"WER: {wer_whisper_small}\")\n",
    "\n",
    "#print(f\"CER: {cer_whisper_small}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters in the model\n",
    "#model_small.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot wer and cer scores vs number of parameters\n",
    "#fig = go.Figure()\n",
    "\n",
    "#fig.add_trace(go.Line(x=[model.num_parameters(), model_large_v3.num_parameters(), model_medium.num_parameters(), model_base.num_parameters(), model_small.num_parameters()], y=[wer, wer_whisper_large_v3, wer_whisper_medium, wer_whisper_base, wer_whisper_small], mode='markers', name='WER'))\n",
    "#fig.add_trace(go.Line(x=[model.num_parameters(), model_large_v3.num_parameters(), model_medium.num_parameters(), model_base.num_parameters(), model_small.num_parameters()], y=[cer_whisper_tiny, cer_whisper_large_v3, cer_whisper_medium, cer_whisper_base, cer_whisper_small], mode='markers', name='CER'))\n",
    "\n",
    "#fig.update_layout(title='WER and CER vs Number of Parameters', xaxis_title='Number of Parameters', yaxis_title='WER and CER')\n",
    "\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance assessment summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(columns=['Model', 'WER', 'CER', 'Number of parameters'])\n",
    "#df = df.append({'Model': 'Whisper-Tiny', 'WER': wer, 'CER': cer_whisper_tiny, 'Number of parameters': model.num_parameters()}, ignore_index=True)\n",
    "#df = df.append({'Model': 'Whisper-Large-V3', 'WER': wer_whisper_large_v3, 'CER': cer_whisper_large_v3, 'Number of parameters': model_large_v3.num_parameters()}, ignore_index=True)\n",
    "#df = df.append({'Model': 'Whisper-Medium', 'WER': wer_whisper_medium, 'CER': cer_whisper_medium, 'Number of parameters': model_medium.num_parameters()}, ignore_index=True)\n",
    "#df = df.append({'Model': 'Whisper-Base', 'WER': wer_whisper_base, 'CER': cer_whisper_base, 'Number of parameters': model_base.num_parameters()}, ignore_index=True)\n",
    "#df = df.append({'Model': 'Whisper-Small', 'WER': wer_whisper_small, 'CER': cer_whisper_small, 'Number of parameters': model_small.num_parameters()}, ignore_index=True)\n",
    "\n",
    "# order by number of parameters\n",
    "#df = df.sort_values(by=['Number of parameters'])\n",
    "\n",
    "# reset index\n",
    "#df = df.reset_index(drop=True)\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning whisper tiny on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class whisper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "    \n",
    "    def forward(self, input_features, decoder_input_ids):\n",
    "        return self.model(input_features, decoder_input_ids=decoder_input_ids)\n",
    "    \n",
    "    def generate(self, input_features):\n",
    "        return self.model.generate(input_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37760256\n"
     ]
    }
   ],
   "source": [
    "# get number of parameters\n",
    "model = whisper()\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    input_features = [model.processor(item['audio'], sampling_rate=16000, return_tensors=\"pt\").input_features.squeeze() for item in batch]\n",
    "    labels = [model.tokenizer(item['text'], return_tensors=\"pt\").input_ids.squeeze() for item in batch]\n",
    "\n",
    "    input_features = pad_sequence(input_features, batch_first=True)\n",
    "    labels = pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return {\"input_features\": input_features, \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(Data['train'], batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valloader = DataLoader(Data['validation'], batch_size=batch_size, shuffle=True, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.9197832295149565, Val Loss: 0.09085851034615189\n",
      "Epoch: 2, Train Loss: 0.016946754531003534, Val Loss: 0.10725139733403921\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    for batch in trainloader:\n",
    "        input_features = batch['input_features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_features, labels)\n",
    "        loss = criterion(outputs.logits.permute(0, 2, 1), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_batch_loss.append(loss.item())\n",
    "    train_loss.append(np.mean(train_batch_loss))\n",
    "    model.eval()\n",
    "    val_batch_loss = []\n",
    "    for batch in valloader:\n",
    "        input_features = batch['input_features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_features, labels)\n",
    "        loss = criterion(outputs.logits.permute(0, 2, 1), labels)\n",
    "        val_batch_loss.append(loss.item())\n",
    "    val_loss.append(np.mean(val_batch_loss))\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
